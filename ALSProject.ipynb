{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nmkqNtWpV4r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-8FNO-KpYlr"
      },
      "source": [
        "Importing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqbSQzQHp7Xe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from PIL import Image\n",
        "import kagglehub\n",
        "\n",
        "\n",
        "# Downloading dataset\n",
        "als_data = kagglehub.dataset_download(\"uraninjo/augmented-alzheimer-mri-dataset\")\n",
        "dataset_path = als_data  # Path where dataset is stored\n",
        "#print(\"Path to dataset files:\", als_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnCtDBSWpdGy"
      },
      "source": [
        "Checking for corrupted files, using a function. Removes images if corrupted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLWbO4BWpg2i"
      },
      "outputs": [],
      "source": [
        "def is_corrupt(image_path):\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        img.verify()  # Check if image is readable\n",
        "        return False\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "# Remove corrupted images\n",
        "image_paths = []\n",
        "for root, _, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        if file.endswith(('jpg', 'png', 'jpeg')):\n",
        "            full_path = os.path.join(root, file)\n",
        "            if not is_corrupt(full_path):\n",
        "                image_paths.append(full_path)\n",
        "            else:\n",
        "                os.remove(full_path)  # Delete corrupted images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_wGoER7p5Gn"
      },
      "source": [
        "Checks images for each class by counting how many images exist for each class. EX: (Early, Mild, Severe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA8qenzWp8Tb",
        "outputId": "ae618621-9784-41a9-b11f-5407b97935c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution: {'AugmentedAlzheimerDataset': 33984, 'OriginalDataset': 6400}\n"
          ]
        }
      ],
      "source": [
        "dataset = datasets.ImageFolder(root=dataset_path)\n",
        "class_counts = {class_name: 0 for class_name in dataset.classes} #dictionary to store class names\n",
        "for _, label in dataset.samples:\n",
        "    class_counts[dataset.classes[label]] += 1\n",
        "print(\"Class Distribution:\", class_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU6qQe0zRSf2"
      },
      "source": [
        "Define transformations for data augmentation & normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUzwCeF3RNwJ"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize images to 128x128 to ensure consistent size;\n",
        "    #needed bc neural networks require fixed sized inputs, also helps with memory effciency\n",
        "    transforms.RandomHorizontalFlip(),  # Random flip for data augmentation\n",
        "    #helps prevent overfitting by increasing the training data, eliminates bias\n",
        "    transforms.RandomRotation(5),  # Small rotation for variation(around 5 degrees)\n",
        "    #makes small variations making the model more robust to slight positional changes\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust brightness & contrast (20%)\n",
        "    #makes the model resilient to real-world conditions where lighting isn't always ideal\n",
        "    transforms.ToTensor(),  # Convert image to PyTorch tensor, changes pixel values from  0-255 to a range of 0-1\n",
        "    #PyTorch models only accept tensors, so this step is essential\n",
        "    #Scaling pixel values from 0-255 to 0-1 helps improve numerical stability during training.\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize pixel values to [-1,1],\n",
        "    #uses newpixel=orignialpixel-mean/std\n",
        "    #Normalization helps stabilize training and speeds up convergence by ensuring that pixel values are centered around zero.\n",
        "    #Many deep learning models (e.g., CNNs) work better when inputs are in a standardized range.\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7qMSSdDRbPB"
      },
      "source": [
        "Load dataset using ImageFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF3oZWxORb1X"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwG4xQBKRefJ"
      },
      "source": [
        "Split dataset into training (80%) and validation (20%)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNX1VIlwRg94"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X181BkTRjxe"
      },
      "source": [
        "Create DataLoaders for batching and shuffling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SejiqEwyRlsx"
      },
      "outputs": [],
      "source": [
        "batch_size = 32  # Adjust if needed based on hardware\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuZcnyQkx_yH"
      },
      "source": [
        "Baseline Model for CNN's"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUIIoLbTRpBa"
      },
      "source": [
        "Print dataset info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAhGIlfhRqwN",
        "outputId": "29b4c0c3-332d-4a97-d39f-6466415f4305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 40384\n",
            "Training images: 32307, Validation images: 8077\n",
            "Classes: ['AugmentedAlzheimerDataset', 'OriginalDataset']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total images: {len(dataset)}\")\n",
        "print(f\"Training images: {len(train_dataset)}, Validation images: {len(val_dataset)}\")\n",
        "print(f\"Classes: {dataset.classes}\")  # Print class labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y8Wfu6pO9d7"
      },
      "source": [
        "## Model Training: Convolutional Neural Networks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KIG71MhHqTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d45745-a507-413f-8a4a-feb73f74a4b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.3662\n",
            "Epoch 2, Loss: 0.2826\n",
            "Epoch 3, Loss: 0.2474\n",
            "Epoch 4, Loss: 0.2202\n",
            "Epoch 5, Loss: 0.2105\n",
            "Training complete. Model saved.\n",
            "Test Accuracy: 0.8618\n",
            "Confusion Matrix:\n",
            "Classification Report:\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "AugmentedAlzheimerDataset       0.87      0.98      0.92      6766\n",
            "          OriginalDataset       0.74      0.23      0.35      1310\n",
            "\n",
            "                 accuracy                           0.86      8076\n",
            "                macro avg       0.80      0.61      0.64      8076\n",
            "             weighted avg       0.85      0.86      0.83      8076\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select device: Use GPU if available, otherwise fallback to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True  # Enable faster training on CUDA\n",
        "\n",
        "# Define the CNN model class\n",
        "class CNNBaseline(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNBaseline, self).__init__()\n",
        "\n",
        "        # First convolutional layer: 3 input channels (RGB), 32 filters, 3x3 kernel\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        # Second convolutional layer: 32 input channels, 64 filters, 3x3 kernel\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        # Max pooling layer with 2x2 kernel and stride of 2 (reduces spatial dimensions by half)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Compute flattened size after two pooling layers\n",
        "        self.flattened_size = 64 * (64 // 4) * (64 // 4)  # 64 filters, image reduced to 16x16\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(self.flattened_size, 128)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(128, num_classes)          # Output layer\n",
        "\n",
        "        # ReLU activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply convolution, ReLU, and pooling for feature extraction\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "\n",
        "        # Flatten the output for the fully connected layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layers with activation\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # Final output (raw scores)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64         # Number of samples per batch\n",
        "epochs = 5              # Total number of training epochs\n",
        "learning_rate = 0.001   # Learning rate for optimizer\n",
        "\n",
        "# Path to dataset (assigned earlier in the notebook)\n",
        "dataset_path = als_data\n",
        "\n",
        "# Define data preprocessing and augmentation pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),                     # Resize all images to 64x64\n",
        "    transforms.RandomHorizontalFlip(),               # Randomly flip images horizontally\n",
        "    transforms.RandomRotation(5),                    # Random rotation within 5 degrees\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Brightness/contrast adjustments\n",
        "    transforms.ToTensor(),                           # Convert image to PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])      # Normalize to range [-1, 1]\n",
        "])\n",
        "\n",
        "# Load dataset from directory with transformations\n",
        "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
        "\n",
        "# Get class names and number of output classes\n",
        "class_names = dataset.classes\n",
        "n_classes = len(class_names)\n",
        "\n",
        "# Use only 20% of dataset to speed up training during development\n",
        "data_subset = int(0.2 * len(dataset))\n",
        "train_dataset, _ = random_split(dataset, [data_subset, len(dataset) - data_subset])\n",
        "test_dataset, _ = random_split(dataset, [int(0.2 * len(dataset)), len(dataset) - int(0.2 * len(dataset))])\n",
        "\n",
        "# Create data loaders for training and testing\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize CNN model and move it to the selected device\n",
        "model = CNNBaseline(num_classes=n_classes).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Cross-entropy for multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to device\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "        running_loss += loss.item()  # Accumulate loss\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Save trained model to disk\n",
        "torch.save(model.state_dict(), 'cnn_baseline.pth')\n",
        "print(\"Training complete. Model saved.\")\n",
        "\n",
        "# Evaluation mode\n",
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "# Disable gradient computation during evaluation\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)  # Forward pass\n",
        "        _, predicted = torch.max(outputs, 1)  # Get predicted class with highest score\n",
        "        predictions.extend(predicted.cpu().numpy())  # Store predictions\n",
        "        true_labels.extend(labels.cpu().numpy())  # Store ground truth labels\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(true_labels, predictions)  # Overall accuracy\n",
        "conf_matrix = confusion_matrix(true_labels, predictions)  # Confusion matrix\n",
        "class_report = classification_report(true_labels, predictions, target_names=class_names)  # Detailed report\n",
        "\n",
        "# Print evaluation results\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "#print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# Optional: visualize the confusion matrix as a heatmap\n",
        "#plt.figure(figsize=(8, 6))\n",
        "#sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "#plt.xlabel('Predicted')\n",
        "#plt.ylabel('Actual')\n",
        "#plt.title('Confusion Matrix')\n",
        "#plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG13LPNWXhhw"
      },
      "source": [
        "# New CNN Model for all four class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UushwX_fXy0l"
      },
      "source": [
        "Class Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XlhO1pzXrcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ccedf2-c2a0-40ee-e8ca-7de21010c3df"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['AugmentedAlzheimerDataset', 'OriginalDataset']\n"
          ]
        }
      ],
      "source": [
        "print(dataset.classes)\n",
        "# Output: ['Mild Demented', 'Moderate Demented', 'Non Demented', 'Very Mild Demented']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EScrVPLUX2W-"
      },
      "source": [
        "Evaluation with Class Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4uD2FYJX6am",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5674bef-d8a4-4760-faba-86fded5745b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8618\n",
            "Confusion Matrix:\n",
            "[[6657  109]\n",
            " [1007  303]]\n",
            "Classification Report:\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "AugmentedAlzheimerDataset       0.87      0.98      0.92      6766\n",
            "          OriginalDataset       0.74      0.23      0.35      1310\n",
            "\n",
            "                 accuracy                           0.86      8076\n",
            "                macro avg       0.80      0.61      0.64      8076\n",
            "             weighted avg       0.85      0.86      0.83      8076\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Class names from dataset\n",
        "class_names = dataset.classes\n",
        "\n",
        "# Compute accuracy and classification report\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "class_report = classification_report(true_labels, predictions, target_names=class_names)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Model For each Specific File"
      ],
      "metadata": {
        "id": "NZgpRKf0tr7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# === STEP 1: Define paths to image folders for each category ===\n",
        "base_dir = 'C:/Users/Facooney99/Downloads/ALSMRIScans.zip/OriginalDataset'\n",
        "category_paths = {\n",
        "    'Mild Demented': os.path.join(base_dir, 'MildDemented'),\n",
        "    'Moderate Demented': os.path.join(base_dir, 'ModerateDemented'),\n",
        "    'Non Demented': os.path.join(base_dir, 'NonDemented'),\n",
        "    'Very Mild Demented': os.path.join(base_dir, 'VeryMildDemented')\n",
        "}\n",
        "\n",
        "# === STEP 2: Function to create image generators for each category ===\n",
        "def create_data_generator(data_dir, target_size=(128, 128), batch_size=32):\n",
        "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "    train_gen = datagen.flow_from_directory(data_dir, target_size=target_size,\n",
        "                                            batch_size=batch_size, class_mode='categorical',\n",
        "                                            subset='training')\n",
        "    val_gen = datagen.flow_from_directory(data_dir, target_size=target_size,\n",
        "                                          batch_size=batch_size, class_mode='categorical',\n",
        "                                          subset='validation')\n",
        "    return train_gen, val_gen\n",
        "\n",
        "# === STEP 3: Function to build a simple CNN model ===\n",
        "def build_cnn_model(input_shape=(128, 128, 3), num_classes=1):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D(2,2),\n",
        "        Conv2D(64, (3,3), activation='relu'),\n",
        "        MaxPooling2D(2,2),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# === STEP 4: Train a CNN for each category ===\n",
        "models = {}\n",
        "histories = {}\n",
        "\n",
        "for category, path in category_paths.items():\n",
        "    print(f\"\\n--- Training CNN for: {category} ---\")\n",
        "    train_gen, val_gen = create_data_generator(path)\n",
        "    model = build_cnn_model(num_classes=train_gen.num_classes)\n",
        "    history = model.fit(train_gen, validation_data=val_gen, epochs=10)\n",
        "\n",
        "    models[category] = model\n",
        "    histories[category] = history\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "qxyDPOfStrO8",
        "outputId": "4b688d76-e1c0-4430-c800-b4f9cabe8cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training CNN for: Mild Demented ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:/Users/Facooney99/Downloads/ALSMRIScans.zip/OriginalDataset/MildDemented'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-51754ed50d7a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategory_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n--- Training CNN for: {category} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_cnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-51754ed50d7a>\u001b[0m in \u001b[0;36mcreate_data_generator\u001b[0;34m(data_dir, target_size, batch_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdatagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     train_gen = datagen.flow_from_directory(data_dir, target_size=target_size,\n\u001b[0m\u001b[1;32m     20\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                             subset='training')\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mkeep_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     ):\n\u001b[0;32m-> 1138\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Facooney99/Downloads/ALSMRIScans.zip/OriginalDataset/MildDemented'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}